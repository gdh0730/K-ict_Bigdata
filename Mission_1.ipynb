{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","authorship_tag":"ABX9TyOSUjoeLNBJNz8aSQO8DQtA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ihDJ8RpkCSVe","executionInfo":{"status":"ok","timestamp":1730357186202,"user_tz":-540,"elapsed":2745,"user":{"displayName":"구동한","userId":"00757618684351472180"}},"outputId":"f59026f8-dc83-4327-d9a3-900453e78608"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["1-1"],"metadata":{"id":"4dwz85E4TtKT"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from collections import defaultdict\n","\n","# 이미지 데이터가 저장된 경로 설정\n","train_dir = \"/content/drive/MyDrive/dataset/training_image\"      # 실제 학습 데이터 디렉토리로 변경하세요\n","val_dir = \"/content/drive/MyDrive/dataset/validation_image\"      # 실제 검증 데이터 디렉토리로 변경하세요\n","\n","# 성별 및 스타일별 이미지 수를 저장할 defaultdict 초기화\n","def count_images(directory):\n","    counts = defaultdict(lambda: defaultdict(int))\n","    for filename in os.listdir(directory):\n","        if filename.endswith('.jpg'):\n","            parts = filename.split('_')\n","            gender = \"여성\" if parts[-1].startswith('W') else \"남성\"\n","            style = parts[-2]\n","            counts[gender][style] += 1\n","    return counts\n","\n","# Training 및 Validation 데이터셋에 대해 각각 카운트 수행\n","train_counts = count_images(train_dir)\n","val_counts = count_images(val_dir)\n","\n","# defaultdict 데이터를 pandas DataFrame으로 변환\n","def dict_to_dataframe(counts):\n","    df = pd.DataFrame(counts).fillna(0).astype(int)\n","    df.index.name = '스타일'\n","    df.columns.name = '성별'\n","    return df\n","\n","train_df = dict_to_dataframe(train_counts)\n","val_df = dict_to_dataframe(val_counts)\n","\n","# 데이터를 원하는 형식으로 변환\n","def reshape_dataframe(df):\n","    melted_df = df.reset_index().melt(id_vars=['스타일'], var_name='성별', value_name='이미지 수')\n","    # 열 순서 변경\n","    melted_df = melted_df[['성별', '스타일', '이미지 수']]\n","    return melted_df\n","\n","# Training 및 Validation 데이터 재구성\n","train_reshaped = reshape_dataframe(train_df)\n","val_reshaped = reshape_dataframe(val_df)\n","\n","# 결과 출력\n","print(\"Training 데이터 통계\")\n","print(train_reshaped)\n","print(\"\\nValidation 데이터 통계\")\n","print(val_reshaped)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e4CT-jYUTuN1","executionInfo":{"status":"ok","timestamp":1730357192229,"user_tz":-540,"elapsed":2226,"user":{"displayName":"구동한","userId":"00757618684351472180"}},"outputId":"7b4bfa3f-70e2-4b23-ad7a-60c42f44426a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training 데이터 통계\n","    성별             스타일  이미지 수\n","0   여성        feminine    154\n","1   여성           space     37\n","2   여성        normcore    153\n","3   여성   bodyconscious     95\n","4   여성      genderless     77\n","5   여성          hiphop     48\n","6   여성          kitsch     91\n","7   여성        lingerie     55\n","8   여성        cityglam     67\n","9   여성        oriental     78\n","10  여성         minimal    139\n","11  여성          hippie     91\n","12  여성  sportivecasual    157\n","13  여성          lounge     45\n","14  여성         classic     77\n","15  여성      athleisure     67\n","16  여성         ecology     64\n","17  여성            punk     65\n","18  여성       powersuit    120\n","19  여성          popart     41\n","20  여성        military     33\n","21  여성           disco     37\n","22  여성          grunge     31\n","23  여성     metrosexual      0\n","24  여성            mods      0\n","25  여성            bold      0\n","26  여성             ivy      0\n","27  남성        feminine      0\n","28  남성           space      0\n","29  남성        normcore    364\n","30  남성   bodyconscious      0\n","31  남성      genderless      0\n","32  남성          hiphop    274\n","33  남성          kitsch      0\n","34  남성        lingerie      0\n","35  남성        cityglam      0\n","36  남성        oriental      0\n","37  남성         minimal      0\n","38  남성          hippie    260\n","39  남성  sportivecasual    298\n","40  남성          lounge      0\n","41  남성         classic      0\n","42  남성      athleisure      0\n","43  남성         ecology      0\n","44  남성            punk      0\n","45  남성       powersuit      0\n","46  남성          popart      0\n","47  남성        military      0\n","48  남성           disco      0\n","49  남성          grunge      0\n","50  남성     metrosexual    278\n","51  남성            mods    269\n","52  남성            bold    268\n","53  남성             ivy    237\n","\n","Validation 데이터 통계\n","    성별             스타일  이미지 수\n","0   여성        cityglam     18\n","1   여성        feminine     44\n","2   여성          grunge     10\n","3   여성        oriental     18\n","4   여성          hippie     14\n","5   여성       powersuit     34\n","6   여성      genderless     12\n","7   여성         minimal     35\n","8   여성  sportivecasual     48\n","9   여성        military      9\n","10  여성          kitsch     22\n","11  여성      athleisure     14\n","12  여성           space     15\n","13  여성   bodyconscious     23\n","14  여성         classic     22\n","15  여성        lingerie      5\n","16  여성            punk     12\n","17  여성           disco     10\n","18  여성         ecology     17\n","19  여성        normcore     20\n","20  여성          popart      8\n","21  여성          hiphop      8\n","22  여성          lounge      8\n","23  여성             ivy      0\n","24  여성            mods      0\n","25  여성     metrosexual      0\n","26  여성            bold      0\n","27  남성        cityglam      0\n","28  남성        feminine      0\n","29  남성          grunge      0\n","30  남성        oriental      0\n","31  남성          hippie     82\n","32  남성       powersuit      0\n","33  남성      genderless      0\n","34  남성         minimal      0\n","35  남성  sportivecasual     52\n","36  남성        military      0\n","37  남성          kitsch      0\n","38  남성      athleisure      0\n","39  남성           space      0\n","40  남성   bodyconscious      0\n","41  남성         classic      0\n","42  남성        lingerie      0\n","43  남성            punk      0\n","44  남성           disco      0\n","45  남성         ecology      0\n","46  남성        normcore     51\n","47  남성          popart      0\n","48  남성          hiphop     66\n","49  남성          lounge      0\n","50  남성             ivy     79\n","51  남성            mods     80\n","52  남성     metrosexual     58\n","53  남성            bold     57\n"]}]},{"cell_type":"markdown","source":["1-2"],"metadata":{"id":"tBPkGqo5Tuw9"}},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, ConcatDataset\n","from sklearn.utils import class_weight\n","from PIL import Image\n","from torchvision import transforms\n","from torchvision.models import resnet18\n","from tqdm import tqdm\n","import numpy as np\n","\n","# 학습 이미지 경로 및 생성 이미지 경로\n","train_dir = '/content/drive/MyDrive/dataset/training_image'\n","cropped_dir = '/content/drive/MyDrive/dataset/cropped_image'\n","val_dir = '/content/drive/MyDrive/dataset/validation_image'\n","\n","# 업데이트된 평균과 표준편차로 데이터 전처리 및 변환 설정\n","transform_train1 = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(p=1.0),      # 좌우 반전 확률 100%\n","    transforms.RandomRotation(degrees=10),       # -10~10도 사이로 회전\n","    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),  # 밝기, 대비, 채도, 색조 조정 (약하게)\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.4933, 0.4610, 0.4464], std=[0.2573, 0.2508, 0.2519])\n","])\n","\n","transform_val = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.4933, 0.4610, 0.4464], std=[0.2573, 0.2508, 0.2519])\n","])"],"metadata":{"id":"KfGCDqogLrqG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 클래스 인덱스 매핑 생성 함수\n","# gender와 style 정보를 조합하여 고유한 클래스 인덱스를 생성\n","def create_class_to_idx(directories):\n","    # 고유한 (gender, style) 쌍을 저장하기 위한 집합\n","    classes = set()\n","    # 주어진 디렉토리 리스트에 대해 반복\n","    for directory in directories:\n","        for filename in os.listdir(directory):\n","            # 이미지 파일이 .jpg로 끝나는 경우만 처리\n","            if filename.endswith('.jpg'):\n","                parts = filename.split('_')  # 파일명에서 gender와 style 정보를 추출하기 위해 '_'로 분리\n","                # 파일명 끝에 'W'가 있는 경우를 여성으로 처리, 'M'이면 남성으로 처리\n","                gender = 0 if parts[-1].startswith('W') else 1  # 여성: 0, 남성: 1\n","                # 스타일 정보는 마지막에서 두 번째 부분에 위치\n","                style = parts[-2]\n","                # gender와 style의 조합을 classes 집합에 추가\n","                classes.add((gender, style))\n","    # (gender, style) 쌍에 고유한 인덱스를 부여하여 class_to_idx 딕셔너리 생성\n","    class_to_idx = {cls: idx for idx, cls in enumerate(sorted(classes))}\n","    return class_to_idx\n","\n","# 이미지 데이터셋 클래스 정의\n","# 주어진 이미지 디렉토리에서 이미지와 라벨을 불러와서 메모리에 저장\n","class CustomImageDataset(Dataset):\n","    def __init__(self, image_dir, class_to_idx, transform=None):\n","        # 이미지와 라벨을 저장할 리스트\n","        self.images = []\n","        self.labels = []\n","        self.class_to_idx = class_to_idx  # (gender, style) 조합과 인덱스 매핑 정보\n","        self.transform = transform  # 이미지에 적용할 전처리 변환\n","\n","        # 주어진 디렉토리에서 파일을 하나씩 처리\n","        for filename in os.listdir(image_dir):\n","            if filename.endswith('.jpg'):\n","                parts = filename.split('_')  # 파일명을 '_'로 분리하여 파트별로 나눔\n","                # 파일명 끝에 'W'가 있는 경우를 여성으로, 'M'이면 남성으로 처리\n","                gender = 0 if parts[-1].startswith('W') else 1  # 여성: 0, 남성: 1\n","                # 스타일 정보는 마지막에서 두 번째 부분\n","                style = parts[-2]\n","                # (gender, style) 쌍을 cls 변수에 저장\n","                cls = (gender, style)\n","                # 클래스 인덱스 매핑에서 해당 (gender, style) 쌍에 해당하는 인덱스를 가져옴\n","                if cls in self.class_to_idx:\n","                    label = self.class_to_idx[cls]  # 고유한 클래스 인덱스\n","\n","                # 유효한 라벨이 있는 경우만 처리\n","                if label is not None:\n","                    # 이미지 경로 생성\n","                    image_path = os.path.join(image_dir, filename)\n","                    # 이미지를 열어 RGB 형식으로 변환한 후 메모리에 로드\n","                    image = Image.open(image_path).convert('RGB')\n","                    # 전처리(transform)가 정의되어 있으면 적용\n","                    if self.transform:\n","                        image = self.transform(image)\n","\n","                    # 메모리에 로드된 이미지를 images 리스트에 추가\n","                    self.images.append(image)\n","                    # 해당 이미지의 라벨을 labels 리스트에 추가\n","                    self.labels.append(label)\n","\n","    def __len__(self):\n","        # 데이터셋의 전체 이미지 수 반환\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        # 주어진 인덱스의 이미지와 라벨을 반환\n","        image = self.images[idx]\n","        label = self.labels[idx]\n","        return image, label\n","\n","# 유효한 클래스 인덱스 매핑 생성\n","# training_image, cropped_image, validation_image 디렉토리의 모든 (gender, style) 조합에 대해 고유 인덱스를 생성\n","class_to_idx = create_class_to_idx([train_dir, cropped_dir, val_dir])\n","\n","# 학습용 및 검증용 데이터셋 생성\n","# training_image 및 cropped_image에서 각각 데이터셋을 생성하고 전처리를 적용\n","original_train_dataset = CustomImageDataset(train_dir, class_to_idx, transform=transform_train1)\n","cropped_train_dataset = CustomImageDataset(cropped_dir, class_to_idx, transform=transform_train1)\n","# ConcatDataset을 사용하여 두 학습용 데이터셋을 하나로 결합\n","train_dataset = ConcatDataset([original_train_dataset, cropped_train_dataset])\n","\n","# 검증용 데이터셋 생성 (전처리 변환을 transform_val로 설정)\n","val_dataset = CustomImageDataset(val_dir, class_to_idx, transform=transform_val)"],"metadata":{"id":"LRa5mpGLkXUo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 각 데이터셋의 고유 라벨 확인\n","print(\"Unique labels in original_train_dataset:\", np.unique(original_train_dataset.labels))\n","print(\"Unique labels in cropped_train_dataset:\", np.unique(cropped_train_dataset.labels))\n","print(\"Unique labels in val_dataset:\", np.unique(val_dataset.labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p1shfop8EnUJ","executionInfo":{"status":"ok","timestamp":1730359774594,"user_tz":-540,"elapsed":833,"user":{"displayName":"구동한","userId":"00757618684351472180"}},"outputId":"c4da6769-6dd2-4ee5-b932-81444ac4d1e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique labels in original_train_dataset: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n"," 24 25 26 27 28 29 30]\n","Unique labels in cropped_train_dataset: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n"," 24 25 26 27 28 29 30]\n","Unique labels in val_dataset: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n"," 24 25 26 27 28 29 30]\n"]}]},{"cell_type":"code","source":["# 로드 시 transform 적용할 수 있도록 새 클래스 정의\n","class TransformableLoadedImageDataset(Dataset):\n","    def __init__(self, data, transform=None):\n","        self.images = data['images']\n","        self.labels = data['labels']\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        #if self.transform:\n","        #    image = self.transform(image)  # 로드 시 transform 적용\n","        label = self.labels[idx]\n","        return image, label\n","\n","# 저장된 데이터셋을 로드하고 transform 적용\n","original_train_data = torch.load('original_train_dataset_detect.pt')\n","cropped_train_data = torch.load('cropped_train_dataset_detect.pt')\n","val_data = torch.load('val_dataset_detect.pt')\n","\n","# TransformableLoadedImageDataset 인스턴스 생성 (transform 적용)\n","original_train_dataset = TransformableLoadedImageDataset(original_train_data, transform=None)\n","cropped_train_dataset = TransformableLoadedImageDataset(cropped_train_data, transform=None)\n","val_dataset = TransformableLoadedImageDataset(val_data, transform=None)\n","\n","train_dataset = ConcatDataset([original_train_dataset, cropped_train_dataset])\n","\n","# 데이터 로더 생성\n","train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0, pin_memory=True)\n","val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=0, pin_memory=True)\n","\n","# 데이터 확인 (선택적으로 추가)\n","print(f\"Train Dataset size: {len(train_loader.dataset)}\")\n","print(f\"Validation Dataset size: {len(val_loader.dataset)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VxLdopr8ofrn","executionInfo":{"status":"ok","timestamp":1730363235713,"user_tz":-540,"elapsed":4854,"user":{"displayName":"구동한","userId":"00757618684351472180"}},"outputId":"1c81fabd-6d20-4222-c96d-2701cafb0933"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-16-44bf0e878f67>:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  original_train_data = torch.load('original_train_dataset_detect.pt')\n","<ipython-input-16-44bf0e878f67>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  cropped_train_data = torch.load('cropped_train_dataset_detect.pt')\n","<ipython-input-16-44bf0e878f67>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  val_data = torch.load('val_dataset_detect.pt')\n"]},{"output_type":"stream","name":"stdout","text":["Train Dataset size: 9731\n","Validation Dataset size: 951\n"]}]},{"cell_type":"code","source":["# ResNet-18 모델 정의 (pretrained=False)\n","model = resnet18(pretrained=False)\n","model.fc = nn.Linear(model.fc.in_features, 31)\n","\n","# 장치 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# 클래스 가중치 계산 및 손실 함수 정의\n","labels = np.concatenate([original_train_dataset.labels, cropped_train_dataset.labels])\n","class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n","class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n","criterion = nn.CrossEntropyLoss(weight=class_weights)\n","\n","# 옵티마이저와 학습률 스케줄러 설정\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n","\n","print(f\"Model is on device: {next(model.parameters()).device}\")\n","\n","# 학습 및 검증 함수\n","def train(model, loader, criterion, optimizer, device):\n","    model.train()\n","    running_loss, correct = 0.0, 0\n","    for inputs, labels in tqdm(loader, desc=\"Training\"):\n","        inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * inputs.size(0)\n","        correct += (outputs.argmax(1) == labels).type(torch.float).sum().item()\n","\n","    return running_loss / len(loader.dataset), correct / len(loader.dataset)\n","\n","def validate(model, loader, criterion, device):\n","    model.eval()\n","    running_loss, correct = 0.0, 0\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(loader, desc=\"Validation\"):\n","            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            correct += (outputs.argmax(1) == labels).type(torch.float).sum().item()\n","\n","    return running_loss / len(loader.dataset), correct / len(loader.dataset)\n","\n","# 학습 및 검증 실행\n","num_epochs = 500\n","for epoch in range(num_epochs):\n","    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n","    val_loss, val_acc = validate(model, val_loader, criterion, device)\n","    scheduler.step(val_loss)\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}]: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n","          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n","\n","# 모델 저장\n","torch.save(model.state_dict(), 'resnet18_fashion_classification_with_crops.pth')"],"metadata":{"id":"MpppnZZZQOPH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"789f64bb-5bfa-4d22-964e-28886e2a93ad"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Model is on device: cuda:0\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:25<00:00,  1.52it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.83it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [1/500]: Train Loss=3.8254, Train Acc=0.0181, Val Loss=3.5012, Val Acc=0.0368\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:25<00:00,  1.50it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.70it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [2/500]: Train Loss=3.4878, Train Acc=0.0227, Val Loss=3.4361, Val Acc=0.0095\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.49it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.70it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [3/500]: Train Loss=3.4623, Train Acc=0.0256, Val Loss=3.4265, Val Acc=0.0126\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.47it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.70it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [4/500]: Train Loss=3.4568, Train Acc=0.0270, Val Loss=3.4398, Val Acc=0.0095\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.46it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.57it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [5/500]: Train Loss=3.4344, Train Acc=0.0217, Val Loss=3.4963, Val Acc=0.0179\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.45it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.62it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [6/500]: Train Loss=3.4290, Train Acc=0.0225, Val Loss=3.4390, Val Acc=0.0315\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.45it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.53it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [7/500]: Train Loss=3.4206, Train Acc=0.0337, Val Loss=3.4316, Val Acc=0.0294\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.45it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.62it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [8/500]: Train Loss=3.4162, Train Acc=0.0226, Val Loss=3.4251, Val Acc=0.0147\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:27<00:00,  1.44it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.54it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [9/500]: Train Loss=3.4090, Train Acc=0.0254, Val Loss=3.4532, Val Acc=0.0126\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:27<00:00,  1.44it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.63it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [10/500]: Train Loss=3.4105, Train Acc=0.0526, Val Loss=3.4228, Val Acc=0.0158\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:27<00:00,  1.44it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.45it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [11/500]: Train Loss=3.4064, Train Acc=0.0380, Val Loss=3.4126, Val Acc=0.0158\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:27<00:00,  1.44it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.50it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [12/500]: Train Loss=3.4040, Train Acc=0.0308, Val Loss=3.4288, Val Acc=0.0179\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.45it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.51it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [13/500]: Train Loss=3.4049, Train Acc=0.0386, Val Loss=8.6306, Val Acc=0.0189\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.45it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.39it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [14/500]: Train Loss=3.4076, Train Acc=0.0268, Val Loss=3.4287, Val Acc=0.0168\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.45it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.33it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [15/500]: Train Loss=3.4076, Train Acc=0.0276, Val Loss=3.4336, Val Acc=0.0158\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.45it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.53it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [16/500]: Train Loss=3.4013, Train Acc=0.0487, Val Loss=3.4242, Val Acc=0.0683\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.45it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.56it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [17/500]: Train Loss=3.4022, Train Acc=0.0628, Val Loss=3.4352, Val Acc=0.0231\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.46it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.60it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [18/500]: Train Loss=3.3950, Train Acc=0.0360, Val Loss=3.5183, Val Acc=0.0179\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.46it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.60it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [19/500]: Train Loss=3.3940, Train Acc=0.0485, Val Loss=3.5859, Val Acc=0.0221\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.47it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.55it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [20/500]: Train Loss=3.3929, Train Acc=0.0348, Val Loss=3.4266, Val Acc=0.0126\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.47it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.46it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [21/500]: Train Loss=3.3919, Train Acc=0.0384, Val Loss=3.4296, Val Acc=0.0137\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.47it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.26it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [22/500]: Train Loss=3.3893, Train Acc=0.0285, Val Loss=3.4224, Val Acc=0.0168\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.47it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.44it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [23/500]: Train Loss=3.3925, Train Acc=0.0268, Val Loss=3.4546, Val Acc=0.0158\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.47it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.53it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [24/500]: Train Loss=3.3866, Train Acc=0.0219, Val Loss=3.4296, Val Acc=0.0137\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.48it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.58it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [25/500]: Train Loss=3.3839, Train Acc=0.0286, Val Loss=3.4578, Val Acc=0.0147\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.48it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [26/500]: Train Loss=3.3867, Train Acc=0.0220, Val Loss=3.4049, Val Acc=0.0158\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.47it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.39it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [27/500]: Train Loss=3.3823, Train Acc=0.0251, Val Loss=3.4046, Val Acc=0.0147\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.47it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.55it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch [28/500]: Train Loss=3.3840, Train Acc=0.0289, Val Loss=3.4440, Val Acc=0.0231\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.48it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [29/500]: Train Loss=3.3906, Train Acc=0.0324, Val Loss=3.5691, Val Acc=0.0252\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.48it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [30/500]: Train Loss=3.3828, Train Acc=0.0303, Val Loss=3.4210, Val Acc=0.0147\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.48it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [31/500]: Train Loss=3.3820, Train Acc=0.0366, Val Loss=3.3921, Val Acc=0.0336\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 39/39 [00:26<00:00,  1.49it/s]\n","Validation: 100%|██████████| 4/4 [00:00<00:00,  4.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [32/500]: Train Loss=3.3853, Train Acc=0.0311, Val Loss=3.4028, Val Acc=0.0189\n"]},{"output_type":"stream","name":"stderr","text":["Training:  74%|███████▍  | 29/39 [00:20<00:06,  1.44it/s]"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Tis1KJowQORZ"},"execution_count":null,"outputs":[]}]}